import scrapy
from scrapy.http import Request
from scrapy.selector import Selector
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from nacs.items import ExampleItem

class nacsSpider(scrapy.Spider):
	name = "nacs"
	allow_domains = [""]
	start_urls = [

		]

	def parse(self, response):
		sel = Selector(response)
		urls = sel.xpath('//div[@class="list_nr"]//li/h2/@href').extract()
		for url in urls:
			yield Request(url, callback=self.parse_item)
		main = response.url[:response.url.rfind('/')]
		page = sel.xpath('//div[@class="page"]//@href').extract()
		next_page = main + page[-2][1:]
		final_page = main + page[-1][1:]
		if response.url != final_page:
			yield Request(next_page,callback=self.parse)

	def parse_item(self, response):
		item = ExampleItem()
		sel = Selector(response)
		item['link'] = response.url
		item['title'] = sel.xpath('//div[@class="news_tit"]/text()').extract()
		data = sel.xpath('//div[@class="news_con"]')
		item['content'] = data.xpath('string(.)').extract()
		

		yield item
